Measuring performance systematically. Micro-benchmarks: isolated operations (function/algorithm). Macro-benchmarks: end-to-end scenarios (realistic load). Methodology: warm-up (JIT compilation, caching), multiple iterations (statistical significance), control variables (background processes, frequency scaling), realistic data. Tools: JMH (Java), Benchmark.js (JS), pytest-benchmark (Python), wrk/ab (HTTP). Metrics: throughput (ops/sec), latency (p50/p95/p99), resource usage (CPU/memory). Pitfalls: dead code elimination, constant folding, cache effects. Compare baseline vs optimized, measure regression Ð² CI.
